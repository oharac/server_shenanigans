---
title: "Server Shenanigans"
format: 
  html:
    theme: pulse
    embed-resources: true
editor: source
---

## Logging in to NCEAS servers

:::{.panel-tabset}

### Log in via web browser

Access the server using `servername.nceas.ucsb.edu` as the URL in the browser.  From that, select the server application you want to use (and then probably bookmark that!)

::: {#fig-htop layout-ncol=2}

![Browser window to access Aurora](img/aurora_web.png){#fig-auroraweb .lightbox}

![RStudio Server on Aurora](img/aurora_web_rstudio.png){#fig-aurorawebrstudio .lightbox}

Accessing Aurora through a web browser
:::



### Log in via `ssh`

Secure Socket Shell (SSH) is a common network protocol to create a secure, encrypted link between two computers, e.g., for remote access or file copying. In Terminal (Mac) or command line (PC):

![](img/ssh_windows.png)

Once you log in successfully, you'll see a command line interface for the server.

![](img/aurora_linux.png)

:::

## Copying files to/from server using `scp`

OpenSSH secure file copy `scp` can transfer files from one computer/server to another via SSH.  You will be prompted to enter your password (I believe you can also embed that in the command but I prefer not to do that...)

:::{.panel-tabset}

### Local to remote

Note, assuming you are doing this from your local computer and reaching out to the server, the default path on the remote server is probably your home directory.  The default path on your local computer is whatever folder you happen to be in at the moment.

* `scp <local file path> <remote username>@<remote server name>`
    * e.g., `scp scp_test.R ohara@aurora.nceas.ucsb.edu`
* `scp <local file path> <remote username>@<remote server name>:<remote file name>`
    * e.g., `scp scp_test.R ohara@aurora.nceas.ucsb.edu:~/test/scp_test2.R` (note, directory must exist!)
    
### Remote to local

Note, assuming you are doing this from your local computer and reaching out to the server, the default path on the remote server is probably your home directory.  The default path on your local computer is whatever folder you happen to be in at the moment.  

Unless you know the IP address of your local computer, you probably want to do this  on your local, reaching out to the server.

* `scp <remote username>@<remote server name>:<remote file name> <local file path>`
    * e.g., `scp ohara@aurora.nceas.ucsb.edu:~/test/scp_test2.R scp_test3.R`

### Remote to remote

Basically the same as local-to-remote (or vice versa), except that the server where your entering the command becomes "local"...  when it asks for a password, it wants the password to the other server.

* `scp <file path on current remote> <other remote username>@<other remote server name>:<other remote filename>`
    * e.g., `scp ~/test/scp_test2.R ohara@mazu.nceas.ucsb.edu:~/github/scp_test2a.R`

:::

## Using `byobu` to manage text-based interactive windows in Linux

When working in Linux it can be helpful to have multiple windows running different operations on a server or computer. The `byobu` app lets you create multiple tabs in the Linux terminal that can be running different processes.

`byobu-enable` to enable `byobu` but doesn't run it yet.

`byobu` to activate `byobu` mode.

-   `F2` to create new tabs
-   `F3` and `F4` to navigate between tabs (leftward or rightward)
-   `F6` to exit `byobu` mode leaving the session intact - next time you enter `byobu` mode it will still have the same tabs available
-   `exit` to close a `byobu` tab (if it's the last tab, it closes the whole session and exits you out to the regular terminal)

![](img/byobu_aurora.png)

## Using `htop` for real time process monitoring

Using `htop` you can see processors and memory in use, and below that a list of processes that are running, including owner, CPU usage, memory usage, etc.

::: {#fig-htop layout-ncol=2}

![`htop` on Mazu (88 cores, 126 GB RAM)](img/htop_mazu.png){#fig-mazu .lightbox}

![`htop` on Aurora (336 cores, 2.2 TB RAM)](img/htop_aurora.png){#fig-aurora .lightbox}

`htop` on Mazu and Aurora
:::

Clicking on the column headers on the process list, you can sort by CPU usage or memory or username (for example). If a process has stalled, you can select the process and kill it (force it to end) - sometimes necessary if a parallel process has failed, leaving orphaned processes stuck. BUT generally not a great position to be in - be very careful!

## Parallelizing code

At least three good options for parallelization in R:

* `parallel` package (base R): `parallel::mclapply()` is a parallelized version of `base::lapply()`
    * `parallel::mclapply(X, FUN, ..., mc.cores)`: `X` is a list or vector over which we want to iterate the `FUN` function (note, the variable passed to `X` must be the first argument in `FUN`); `...` is additional arguments to `FUN` (e.g., `na.rm = TRUE`); `mc.cores` is the number of cores to be used in parallel.
    * Note, this DOES NOT work on Windows... but will work on Linux servers and Mac OSX.
* `furrr` package: a set of `future_map()` functions analogous to `purrr` functions
    * `furrr::future_map()`, `future_map2()`, etc.
    * e.g., `future_map(.x, .f, ..., .options = furrr_options())` where `.x` is the list/vector, `.f` is the function, and `furrr_options()` lets you set up options for the parallel workers.
    * If you're familiar with `purrr` then this would be a good way to go.
* `doParallel` and `foreach` packages: `%dopar%` function will apply parallel execution of `%do%` infix function.
    * I haven't used this one.  Requires separately registering a parallel backend (with `doParallel::registerDoParallel`).  Doesn't look very intuitive...

### Example with `parallel::mclapply()`

Let's create a function that is overly complicated, so it'll take a little time to run and use up a little memory.  This function will take a value `x` and create a big sample from a normal distribution, mean `x`, sd `x/2`.  Then it will find the mean and standard deviation of that sample, and return that as a three-element dataframe.  

```{r}
do_stuff <- function(x, n = 1e8) {
  rand_vec <- rnorm(n, mean = x, sd = x/2)
  mean_x <- mean(rand_vec)
  sd_x   <- sd(rand_vec)
  return(data.frame(x = x, mean = mean_x, sd = sd_x))
}
### on home PC:
# system.time(y <- do_stuff(x = 9))
#   user  system elapsed 
#   5.02    0.12    5.25
# y
#   x     mean       sd
# 1 9 9.000173 4.500088
```

A standard `for` loop on 4 elements takes 4 times as long

```{r}
system.time({
out_list <- list(1, 2, 3, 4) ### initialize an output vector
  for(i in 1:4) {
    out_list[[i]] <- do_stuff(x = i)
  }
})
### on home PC: 4 times as long
#  user  system elapsed 
# 18.33    0.44   19.41 
dplyr::bind_rows(out_list)
```

A standard `lapply` does too:

```{r}
system.time({
  out_list <- lapply(X = 1:4, FUN = do_stuff)
})
  #  user  system elapsed 
  # 18.51    0.33   19.66 
dplyr::bind_rows(out_list)
```

